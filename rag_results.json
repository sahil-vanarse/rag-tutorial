[
  {
    "chunk_id": 1,
    "enhanced_content": "**Transformer Model Variations: English-to-German Translation**\n\n**Key Facts, Numbers, and Data Points:**\n\n* **Model Variations:** The Transformer model was varied in different ways to evaluate the importance of different components.\n* **Performance Metrics:** English-to-German translation performance was measured using perplexity (PPL) and BLEU scores on the newstest2013 development set.\n* **Model Sizes:** Models with different sizes were tested, including 6, 16, 32, 256, and 1024 attention heads, and 512, 1024, and 4096 hidden dimensions.\n* **Attention Key and Value Dimensions:** The attention key and value dimensions were varied, with values of 32, 64, and 128.\n* **Dropout Rates:** Dropout rates of 0.1 and 0.2 were tested.\n* **Training Steps:** Models were trained for 100K and 300K steps.\n* **Perplexity (PPL) Scores:** PPL scores ranged from 4.33 to 5.77.\n* **BLEU Scores:** BLEU scores ranged from 23.7 to 26.4.\n* **Parameters:** Model parameters ranged from 65 to 168 million.\n\n**Main Topics and Concepts:**\n\n* **Transformer Model Variations:** The Transformer model was varied in different ways to evaluate the importance of different components.\n* **English-to-German Translation:** English-to-German translation performance was measured using perplexity (PPL) and BLEU scores on the newstest2013 development set.\n* **Attention Mechanism:** The attention mechanism was varied, including the number of attention heads and attention key and value dimensions.\n* **Model Size:** Models with different sizes were tested, including 6, 16, 32, 256, and 1024 attention heads, and 512, 1024, and 4096 hidden dimensions.\n* **Dropout:** Dropout rates of 0.1 and 0.2 were tested.\n* **Training Steps:** Models were trained for 100K and 300K steps.\n* **Positional Encoding:** Learned positional embeddings were used instead of sinusoidal positional encoding.\n\n**Questions this Content Could Answer:**\n\n* What are the key components of the Transformer model that affect English-to-German translation performance?\n* How do different model sizes and attention mechanisms affect translation performance?\n* What is the impact of dropout rates on translation performance?\n* How do different training steps and perplexity scores affect translation performance?\n* What are the advantages and disadvantages of using learned positional embeddings instead of sinusoidal positional encoding?\n\n**Visual Content Analysis:**\n\n* **Table 3:** The table shows the results of varying different components of the Transformer model, including model size, attention mechanism, and dropout rates.\n* **Perplexity (PPL) Scores:** The PPL scores range from 4.33 to 5.77, indicating a significant variation in translation performance.\n* **BLEU Scores:** The BLEU scores range from 23.7 to 26.4, indicating a significant variation in translation performance.\n* **Model Parameters:** The model parameters range from 65 to 168 million, indicating a significant variation in model complexity.\n\n**Alternative Search Terms:**\n\n* **Transformer Model Variations**\n* **English-to-German Translation**\n* **Attention Mechanism**\n* **Model Size**\n* **Dropout**\n* **Training Steps**\n* **Positional Encoding**\n* **Perplexity (PPL) Scores**\n* **BLEU Scores**\n* **Model Parameters**\n* **Learned Positional Embeddings**\n* **Sinusoidal Positional Encoding**\n\n**Searchable Description:**\n\nTransformer Model Variations: English-to-German Translation\n\nKey Facts, Numbers, and Data Points:\n\n* Model Variations: The Transformer model was varied in different ways to evaluate the importance of different components.\n* Performance Metrics: English-to-German translation performance was measured using perplexity (PPL) and BLEU scores on the newstest2013 development set.\n* Model Sizes: Models with different sizes were tested, including 6, 16, 32, 256, and 1024 attention heads, and 512, 1024, and 4096 hidden dimensions.\n* Attention Key and Value Dimensions: The attention key and value dimensions were varied, with values of 32, 64, and 128.\n* Dropout Rates: Dropout rates of 0.1 and 0.2 were tested.\n* Training Steps: Models were trained for 100K and 300K steps.\n* Perplexity (PPL) Scores: PPL scores ranged from 4.33 to 5.77.\n* BLEU Scores: BLEU scores ranged from 23.7 to 26.4.\n* Parameters: Model parameters ranged from 65 to 168 million.\n\nMain Topics and Concepts:\n\n* Transformer Model Variations\n* English-to-German Translation\n* Attention Mechanism\n* Model Size\n* Dropout\n* Training Steps\n* Positional Encoding\n\nQuestions this Content Could Answer:\n\n* What are the key components of the Transformer model that affect English-to-German translation performance?\n* How do different model sizes and attention mechanisms affect translation performance?\n* What is the impact of dropout rates on translation performance?\n* How do different training steps and perplexity scores affect translation performance?\n* What are the advantages and disadvantages of using learned positional embeddings instead of sinusoidal positional encoding?\n\nVisual Content Analysis:\n\n* Table 3: The table shows the results of varying different components of the Transformer model, including model size, attention mechanism, and dropout rates.\n* Perplexity (PPL) Scores: The PPL scores range from 4.33 to 5.77, indicating a significant variation in translation performance.\n* BLEU Scores: The BLEU scores range from 23.7 to 26.4, indicating a significant variation in translation performance.\n* Model Parameters: The model parameters range from 65 to 168 million, indicating a significant variation in model complexity.\n\nAlternative Search Terms:\n\n* Transformer Model Variations\n* English-to-German Translation\n* Attention Mechanism\n* Model Size\n* Dropout\n* Training Steps\n* Positional Encoding\n* Perplexity (PPL) Scores\n* BLEU Scores\n* Model Parameters\n* Learned Positional Embeddings\n* Sinusoidal Positional Encoding",
    "metadata": {
      "original_content": {
        "raw_text": "6.2 Model Variations\n\nTo evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\n\n8\n\nTable 3: Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.\n\nN dmodel dff h dk dv Pdrop ϵls train steps PPL (dev) BLEU params (dev) ×106 base 6 512 2048 8 64 64 0.1 0.1 100K 4.92 25.8 65 1 512 512 5.29 24.9 (A) 4 16 128 32 128 32 5.00 4.91 25.5 25.8 32 16 16 5.01 25.4 (B) 16 32 5.16 5.01 25.1 25.4 58 60 2 6.11 23.7 36 4 5.19 25.3 50 8 4.88 25.5 80 (C) 256 32 32 5.75 24.5 28 1024 128 128 4.66 26.0 168 1024 5.12 25.4 53 4096 4.75 26.2 90 0.0 5.77 24.6 (D) 0.2 0.0 4.95 4.67 25.5 25.3 0.2 5.47 25.7 (E) positional embedding instead of sinusoids 4.92 25.7\n\nbig\n\n6\n\n1024\n\n4096\n\n16\n\n0.3\n\n300K 4.33\n\n26.4\n\ndevelopment set, newstest2013. We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3.\n\nIn Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads.\n\nIn Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model.",
        "tables_html": [
          "<table><thead><tr><th></th><th>N</th><th>dyoast</th><th>de</th><th>Rh</th><th>de</th><th>dy</th><th>Parop</th><th>ets</th><th>Game</th><th>| deny</th><th></th><th>dev).</th></tr></thead><tbody><tr><td>base</td><td>| 6</td><td>512</td><td>2048</td><td>8</td><td>64</td><td>64</td><td>0.1</td><td>O01</td><td>100K</td><td>| 4.92</td><td>25.8</td><td>65</td></tr><tr><td rowspan=\"4\">(A)</td><td></td><td></td><td></td><td>1</td><td>512</td><td>512</td><td></td><td></td><td></td><td>5.29</td><td>24.9</td><td></td></tr><tr><td></td><td></td><td></td><td>4</td><td>128</td><td>128</td><td></td><td></td><td></td><td>5.00</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td>16</td><td>32</td><td>32</td><td></td><td></td><td></td><td>491</td><td>25.8</td><td></td></tr><tr><td></td><td></td><td></td><td>32</td><td>16 =</td><td>16</td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td></td></tr><tr><td rowspan=\"2\">(B)</td><td></td><td></td><td></td><td></td><td>16</td><td></td><td></td><td></td><td></td><td>5.16</td><td>9 25.1</td><td>58</td></tr><tr><td></td><td></td><td></td><td></td><td>32</td><td></td><td></td><td></td><td></td><td>5.01</td><td>25.4</td><td>60</td></tr><tr><td rowspan=\"7\">(C)</td><td>2</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>6.11</td><td>23.7</td><td>36</td></tr><tr><td>4</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.19</td><td>25.3</td><td>50</td></tr><tr><td>8</td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.88</td><td>25.5</td><td>80</td></tr><tr><td></td><td>256</td><td></td><td></td><td>3232</td><td></td><td></td><td></td><td></td><td>5.75</td><td>24.5</td><td>28</td></tr><tr><td></td><td>1024</td><td></td><td></td><td>128</td><td>128</td><td></td><td></td><td></td><td>4.66</td><td>26.0</td><td>168</td></tr><tr><td></td><td></td><td>1024</td><td></td><td></td><td></td><td></td><td></td><td></td><td>5.12</td><td>25.4</td><td>53</td></tr><tr><td></td><td></td><td>4096</td><td></td><td></td><td></td><td></td><td></td><td></td><td>4.75</td><td>26.2</td><td>90</td></tr><tr><td rowspan=\"4\">()</td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td></td><td>5.77</td><td>24.6</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td></td><td>4.95</td><td>25.5</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.0</td><td></td><td>467</td><td>25.3</td><td></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td>0.2</td><td></td><td>5.47</td><td>25.7</td><td></td></tr><tr><td>(E)</td><td></td><td></td><td>positional</td><td>embedding</td><td></td><td>instead of</td><td>sinusoids</td><td></td><td></td><td>4.92</td><td>25.7</td><td></td></tr><tr><td>big</td><td>| 6</td><td>1024</td><td>4096</td><td>16</td><td></td><td></td><td>0.3</td><td></td><td>300K</td><td>| 4.33</td><td>26.4</td><td>213</td></tr></tbody></table>"
        ],
        "images_base64": []
      }
    }
  },
  {
    "chunk_id": 2,
    "enhanced_content": "3.2.3 Applications of Attention in our Model\n\nThe Transformer uses multi-head attention in three different ways:\n\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.",
    "metadata": {
      "original_content": {
        "raw_text": "3.2.3 Applications of Attention in our Model\n\nThe Transformer uses multi-head attention in three different ways:\n\n• In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9].\n\n• The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder.\n\n• Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections. See Figure 2.",
        "tables_html": [],
        "images_base64": []
      }
    }
  },
  {
    "chunk_id": 3,
    "enhanced_content": "7 Conclusion\n\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n\nThe code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.",
    "metadata": {
      "original_content": {
        "raw_text": "7 Conclusion\n\nIn this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.\n\nFor translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.\n\nWe are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours.\n\nThe code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.\n\nAcknowledgements We are grateful to Nal Kalchbrenner and Stephan Gouws for their fruitful comments, corrections and inspiration.",
        "tables_html": [],
        "images_base64": []
      }
    }
  }
]